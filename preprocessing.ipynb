{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "from IPython.display import clear_output\n",
    "import requests\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "import concurrent.futures\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress):\n",
    "    bar_length = 50\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Progress: [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a9803",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, idx):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove \"&nbsp\"\n",
    "    text = re.sub(r\"\\&nbsp\", \"\", text)\n",
    "    # remove urls\n",
    "    # source: url_extract_pattern from https://uibakery.io/regex-library/url-regex-python\n",
    "    url_extract_pattern = \"https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
    "    text = re.sub(url_extract_pattern, '', text)\n",
    "    # remvoe \"\\n\"\n",
    "    text = re.sub(r\"[^ ]*\\n\", \"\", text)\n",
    "    # remove file names with commom endings with 4 or 3 digits\n",
    "    text = re.sub(r\"[^ ]*\\..{4}|[^ ]*\\..{3}\", \"\", text)\n",
    "    # remove any refs\n",
    "    text = re.sub(r\"[^ ]*ref\", \"\", text)\n",
    "    # remove -\n",
    "    text = re.sub(r\"-\", \"\", text)\n",
    "    #remove punctuation thats left\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = text.split(\" \")\n",
    "    # source: https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "    text = [word for word in text if word not in german_stopwords]\n",
    "    \n",
    "    # lemmatization\n",
    "    text_lemma = []\n",
    "\n",
    "    for ix, word in enumerate(text):\n",
    "        doc = nlp(word)\n",
    "        result = ' '.join([x.lemma_ for x in doc]) \n",
    "        text_lemma.append(result)\n",
    "    \n",
    "    final = [gensim.utils.simple_preprocess(word, deacc = True) for word in text_lemma]\n",
    "    \n",
    "    for word in final:\n",
    "        if len(word) == 0:\n",
    "            final.remove(word)\n",
    "    \n",
    "    preprocessed_content.update({idx: final})\n",
    "    #preprocessed_content.append(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ae892",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://de.wikipedia.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487adfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "german_stopwords = stopwords.words(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6236171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request excellent arictles from german wikipedia via wiki api (10 at a time)\n",
    "S = requests.Session()\n",
    "\n",
    "params = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"revisions\",\n",
    "    \"rvprop\": \"content\",\n",
    "    \"rvslots\": \"*\",\n",
    "    \"format\": \"json\",\n",
    "    \"formatversion\": 2,\n",
    "    \"srsearch\": \"incategory:Wikipedia:Exzellent\",\n",
    "    \"list\": \"search\",\n",
    "    \"sroffset\": 0\n",
    "}\n",
    "\n",
    "response = S.get(url = URL, params = params)\n",
    "data = response.json()\n",
    "\n",
    "# get ids from excellent articles\n",
    "ids = []\n",
    "\n",
    "for entry in data[\"query\"][\"search\"]:\n",
    "    ids.append(entry[\"pageid\"])\n",
    "\n",
    "while data.get(\"continue\"):\n",
    "    params.update({\"sroffset\": data[\"continue\"][\"sroffset\"]})\n",
    "    \n",
    "    #print(\"\\n%s\" % (PARAMS))\n",
    "    response = S.get(url = URL, params = params)\n",
    "    data = response.json()\n",
    "    \n",
    "    for entry in data[\"query\"][\"search\"]:\n",
    "        ids.append(entry[\"pageid\"])\n",
    "\n",
    "print(\"Anzahl gesammelter Exzellenter Artikel: %s\" %(len(ids)))\n",
    "\n",
    "#if DATA['query']['search'][0]['title'] == SEARCHPAGE:\n",
    "#    print(\"Your search page '\" + SEARCHPAGE + \"' exists on English Wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request data to every excellent article in german wikipedia via wikipedia api using pageid\n",
    "params = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"revisions\",\n",
    "    \"rvprop\": \"content\",\n",
    "    \"rvslots\": \"*\",\n",
    "    \"format\": \"json\",\n",
    "    \"formatversion\": 2,\n",
    "    \"pageids\": 0\n",
    "}\n",
    "\n",
    "data = pd.DataFrame()\n",
    "content = {}\n",
    "\n",
    "for ix, id in enumerate(ids):\n",
    "    update_progress(ix / len(ids))\n",
    "    params.update({\"pageids\": id})\n",
    "    response = S.get(url = URL, params = params)\n",
    "    page = response.json()\n",
    "    \n",
    "    \"\"\"preprocessed = preprocess(page[\"query\"][\"pages\"][0][\"revisions\"][0][\"slots\"][\"main\"][\"content\"], german_stopwords)\n",
    "    \n",
    "    for word in preprocessed:\n",
    "        if len(word) == 0:\n",
    "            preprocessed.remove(word)\n",
    "        \n",
    "    content.update({id: preprocessed})\"\"\"\n",
    "    content.update({id: page[\"query\"][\"pages\"][0][\"revisions\"][0][\"slots\"][\"main\"][\"content\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c38e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages_by_id(id):\n",
    "    params.update({\"pageids\": id})\n",
    "    response = S.get(url = URL, params = params)\n",
    "    page = response.json()\n",
    "    content.update({id: page[\"query\"][\"pages\"][0][\"revisions\"][0][\"slots\"][\"main\"][\"content\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20e75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = {}\n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(get_pages_by_id, ids)\n",
    "\n",
    "end = time.time()\n",
    "dur = end - start\n",
    "print(\"DUR: %s\" % (dur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save requested data in df\n",
    "df = pd.DataFrame(content.items())\n",
    "df = df.rename({0: \"pageid\", 1:\"content\"}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dd504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fef02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_content = {}\n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(preprocess, df[\"content\"], ids)\n",
    "    \n",
    "end = time.time()\n",
    "dur = end - start\n",
    "print(\"DUR: %s\" % (dur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aea7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data to csv for faster loading\n",
    "#df.to_csv(\"./preprocessed_excellent_article-109.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e0cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [gensim.utils.simple_preprocess(word, deacc = True) for word in test_page_lemma]\n",
    "\n",
    "id2word = corpora.Dictionary(final)\n",
    "\n",
    "corpus = [id2word.doc2bow(word) for word in final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41a28d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(ids) * 53.56256318092346) / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc4d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3108jvsc74a57bd0b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
